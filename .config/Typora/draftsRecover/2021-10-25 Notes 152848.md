# Machine Learning
![Information](../Information.md)

## Introduction

- Solve problems that contain patterns and cannot be solved analytically
- Extract a description of the pattern
- Components of learning
  - Data $x \in D$
  - Target Outputs $y \in Y$
  - Target Function $f : D \rightarrow Y$
  - Experience $x$, (observations, sample data)
  - $x \in X \subset D$
  - $x$ is the input $y$ is the result
  - Predictors, Models, Hypothesis $g : X \rightarrow Y$, $g$ approximates $f$
  - $x$ may be an input image and $y$  may be the binary result of wether or not it contains a face
  - Hypothesis class is the space of all models $H \subset {h : X \rightarrow Y}$
  - Learning algorithm: Goal $g \approx f $
  - Need a way to measure how close $g$ is to $f$
    - ${I : (g,Y) \rightarrow R^+ }$
    - $\hat{I}[g(w, x),f(x)] \in R^+$
    - $\hat{R_n} = \frac{1}{n}\sum^n_{i=1}\hat{I}[g(w, x),f(x)]$
    - Take the sum of the error across all examples

## Learning Tasks

- Classification
  - Binary
  - Multi-class
  - Multi-label
- Regression
  - Uni-variate (continuous)
  - Multivariate (continuous)
- Clustering
  - ![image-20211012135918476](Notes.assets/image-20211012135918476.png)
  - Looking for patterns without a clear metric
  - Grouping based on certain similarities
  - Can be Hierarchical or Flat, (Hierarchical clusters belong to larger sets and so on)
  - Uses
- Anomaly detection
- Dimensionality reduction
- Feature Selection

### Types of learning

##### Supervised Learning

- Learn explicitly 

- Data has clearly defined input-output pairs

- Predict outcome, future on unobserved samples

- Direct feedback is given, prediciton error

- Uses

  - Classification

    - Spam detection
    - Diagnostics
    - Fraud Detection
    - Image/audio/text classification

  - Regression

    - Risk assessment
    - Score prediction
    - Energy consumption

  - Ranking

    - Search engines

    - Information retrieval

    - Machine translation

##### Unsupervised Learning

- Attempts to understand the data, find patterns/ structures

- Training data does not contain the output

- Clustering, association mining, Dimensionality reduction

- Uses:

  - Dimensionality reduction
    - Big data analytics
  - Clustering
    - Biology
    - City Planning
    - Marketing

##### Reinforcement Learning

- Learns how to act in a given environment to maximise rewards
- Inputs, some output, and some valuation is needed
- For instance in games, exact reward of next move is not know
- Decision process, reward and recommendation system
- Uses:
  - Gaming
  - Finance
  - Manufacturing
  - Inventory Management
  - Robot Navigation

#### Categorisation of Learning Types

- Passive vs active learners
  - Data is given vs learner actively decides which data to use during training
  - Oblivious vs adveraril teacher
    - Randomly sampled data vs sleceted hard samples
  - Batch vs online learning
    - All data available vs trainaing and testing with time

### Types of Data

- Structured

  - Organised into specific types
  - Can be used as direct inputs into ML algorithms

- Unstructured

  - Raw data
  - Different types and modalities
  - Needs preprocessing for ML algorithm

  ![image-20211012142024284](Notes.assets/image-20211012142024284.png)

## ML problem formulation

- Input : $x \in X \subset D$
  - text, image, audio, sensor converted into vector representation
- Output: $y \subset Y$
  - label, output value
- Target function $f : X \rightarrow Y$
  - Actual function - never known in practise
- Data: $(x_1,y_1)$ ... $(x_n,y_n)$
  - available for training
- Error Function: $\hat{R_n} = \frac{1}{n}\sum_n\hat{I}(g(w,x),y)$
  - Average over $n$ examples
  - $w$ is a vector of parameters that can be adjusted to decrease error
- Goal $g \approx f$, using error function

## Simple Hypothesis Class - Linear Predictor

**Linear Predictor**:

Input $x = (x_1,...,x_d)$

Hypothesis $w = (x_1,...,x_d)$ (model parameters)

Linear predictor $h(x,w) \rightarrow y$ with $\sum^d_{i=1}w_ix_i = w^Tx$

**Classification**

If result is less than threshold then give true result otherwise give false result

 $$\sum^d_{i=1}w_ix_i - t \ge 0$$

![image-20211012144907306](Notes.assets/image-20211012144907306.png)

### Perception Learning Algorithm

While there exists a missed characterised training point update $w$
$$
y_i\cdot w'^Tx_i = y_i \cdot(w+y_ix_i)^Tx_i \\ 
= y_i\cdot w^Tx_i+y_i^2\cdot x_u^Tx_i \\
= y_i\cdot w^Tx_i+|x_i|^2
$$
Algorithm will stop after a finite number of steps if the data is seperable

## Learning Process of ML
-  Terms
	-  $A$ is impossible $P(A) = 0$
	-  $A$ is certain $P(A) = 1$
	-  $A$ is correct if error $\epsilon_A=0$
	-  $A$ is approximately correct $\epsilon_A \approx 0$
	-  $A$ is probable $P(\epsilon_A=0) \approx 1$
-  Components of learning
	-  Data (training and test data)
	-  Target Function (taken from hypothesis class based on parameters $w$)
	-  Target Output
-  Hoeffding's inequality: $P(|R(h)-\hat{R}_n(h)| > \epsilon) \le 2e^{-2\epsilon^2n}$
   -  Gives a bound on the probability that the difference between the test error and the training error is greater than some epsilon for a given number of samples
   -  Greater number of samples gives lower probability
- **VC Dimension**
	- $H$ shatter $C$ if the set family $H$ contains all subsets of the set $C$.
		- $|H \cap C| = 2^{|C|}$
	- VC dimension of $H$ is the largest cardenality of sets shattered by $H$
		- (Cardenality is the number of elements in a set)
-  Vapnik-Chervonenkis: $P(sup_{h \in H}|\hat{R}_n(h)-R(h)| > \epsilon) \le 4m_H(2n)e^{-\epsilon^2n/8}$
-  Loss function: $l(\hat{y},y)$
   -  Squared error: $l(\hat{y},y) = (\hat{y}-y)^2$
-  Training error: $\hat{R}_n(h) = \frac{1}{n}\sum^n_{i=1}l(h(x_i),y_i)$
-  Test error: $R(h) = E[l(h(x),y)]$
   -  This is expected loss for a hypothesis given all possible input points, this is unknown
-  Can create a loss function to penalise false positives or false negatives more

### Noise and Probabilistic Outcome

- Instead of $y = f(x)$ we can have $y = f(x) + Noise$
- Deterministic is the special case where there is no noise
- Optimal $h$ depends on the noise and the loss function
  - Squared error minimises $E[(y-h(x))^2|x]$
- 

 

